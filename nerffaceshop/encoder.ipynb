{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../external_dependencies')\n",
    "\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "np.int = np.int_\n",
    "np.float = np.float_\n",
    "np.complex = np.complex_\n",
    "np.object = np.object_\n",
    "np.unicode = np.unicode_\n",
    "np.str = np.str_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5df75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dnnlib\n",
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "from typing import List, Union\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def render_tensor(img: torch.Tensor, normalize: bool = True, nrow: int = 8) -> Image.Image:\n",
    "    if type(img) == list:\n",
    "        img = torch.cat([i if len(i.shape) == 4 else i[None, ...] for i in img], dim=0).expand(-1, 3, -1, -1)\n",
    "    elif len(img.shape) == 3:\n",
    "        img = img.expand(3, -1, -1)\n",
    "    elif len(img.shape) == 4:\n",
    "        img = img.expand(-1, 3, -1, -1)\n",
    "    \n",
    "    img = img.squeeze()\n",
    "    \n",
    "    if normalize:\n",
    "        img = img / 2 + .5\n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        return Image.fromarray((img.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
    "    elif len(img.shape) == 2:\n",
    "        return Image.fromarray((img.cpu().numpy() * 255).astype(np.uint8))\n",
    "    elif len(img.shape) == 4:\n",
    "        return Image.fromarray((make_grid(img, nrow=nrow).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(img: Union[Image.Image, np.ndarray], normalize=True) -> torch.Tensor:\n",
    "    if isinstance(img, Image.Image):\n",
    "        img = np.array(img)\n",
    "        if len(img.shape) > 2:\n",
    "            img = img.transpose(2, 0, 1)\n",
    "        else:\n",
    "            img = img[None, ...]\n",
    "    else:\n",
    "        if img.shape[0] == img.shape[1]:\n",
    "            img = img.transpose(2, 0, 1)\n",
    "    if normalize:\n",
    "        img = torch.from_numpy(img).to(torch.float32) / 127.5 - 1\n",
    "    else:\n",
    "        img = torch.from_numpy(img).to(torch.float32) / 255.\n",
    "    return img[None, ...].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58105d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random as r\n",
    "from camera_utils import LookAtPoseSampler, FOV_to_intrinsics\n",
    "from torch_utils import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from external_dependencies.decalib import DECAWrapper\n",
    "deca = DECAWrapper(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee31995",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/vfhq-celebv-text-64.pkl\", \"rb\") as f:\n",
    "    G = pickle.load(f)[\"G_ema\"].to(device).eval().requires_grad_(False)\n",
    "G.exp_mask = (torch.from_numpy(np.array(Image.open('../data/plane_0.png').convert('L'))).to(torch.float32) / 255.)[None, None, :, :].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a245ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fov_deg = 18.837\n",
    "intrinsics = FOV_to_intrinsics(fov_deg, device=device)\n",
    "cam_pivot = torch.tensor(G.rendering_kwargs.get('avg_camera_pivot', [0, 0, 0]), device=device)\n",
    "cam_radius = G.rendering_kwargs.get('avg_camera_radius', 2.7)\n",
    "conditioning_cam2world_pose = LookAtPoseSampler.sample(np.pi/2, np.pi/2, cam_pivot, radius=cam_radius, device=device)\n",
    "conditioning_params = torch.cat([conditioning_cam2world_pose.reshape(-1, 16), intrinsics.reshape(-1, 9)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327db26f-ba70-4c2b-aad6-2c112da6b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor([ 0.8214,  0.0908,  0.3353, -0.1008,  0.1011,  0.1123, -0.1217, -0.1401, 0.0878])[None, ...].to(device)\n",
    "wl = G.backbone.lmapping(None, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.encoder import Encoder\n",
    "encoder = Encoder(50 + 3, 64, 128, 3, 5).eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c23404",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.train().requires_grad_(True)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24eea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeffs(image):\n",
    "    codedict = deca.encode(render_tensor(image.clamp(-1, 1)))\n",
    "    return torch.cat((codedict['exp'], codedict['pose'][:, 3:]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d129c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_get_coeffs(images):\n",
    "    return torch.cat([get_coeffs(image) for image in images.unbind(dim=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e368cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lpips import LPIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lpips_fn = LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_s = []\n",
    "step = 0\n",
    "while True:\n",
    "    z = torch.randn(batch_size, 512).to(device)\n",
    "    w = G.backbone.mapping(z, conditioning_params.expand(batch_size, -1), 1.) # .5\n",
    "    \n",
    "    d = torch.randn(batch_size, G.d_dim).to(device)\n",
    "    wd = G.backbone.dmapping(d, None)\n",
    "    \n",
    "    out = G.synthesis(\n",
    "        w, wd, wl.expand(batch_size, -1, -1), conditioning_params.expand(batch_size, -1), \n",
    "        use_exp_mask=True\n",
    "    )\n",
    "    try:\n",
    "        coeffs = batch_get_coeffs(out[\"image\"])\n",
    "    except:\n",
    "        continue\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred_wd = encoder(coeffs)[:, None, :] + G.backbone.dmapping.w_avg[None, None, :]\n",
    "    # pred_wd = G.backbone.dmapping(pred_d, None)\n",
    "    pred_out = G.synthesis(\n",
    "        w, pred_wd.expand_as(wd), wl.expand(batch_size, -1, -1), conditioning_params.expand(batch_size, -1), \n",
    "        use_exp_mask=True\n",
    "    )\n",
    "    \n",
    "    image_loss = torch.nn.L1Loss()(out[\"image\"], pred_out[\"image\"]) + lpips_fn(out[\"image\"], pred_out[\"image\"]).mean()\n",
    "    code_loss = 1. - torch.nn.CosineSimilarity(dim=-1)(wd[:, 0, :], pred_wd[:, 0, :]).mean() # + torch.nn.L1Loss()(d, pred_d)\n",
    "    \n",
    "    # print(image_loss, code_loss)\n",
    "    \n",
    "    loss = image_loss + code_loss * 0.1\n",
    "    loss.backward()\n",
    "    loss_s.append(float(loss))\n",
    "    optimizer.step()\n",
    "    \n",
    "    step += 1\n",
    "    \n",
    "    if step % 100 == 0 or step == 1:\n",
    "        clear_output(wait=True)\n",
    "        print(step)\n",
    "        plt.plot(loss_s[-100:])\n",
    "        plt.show()\n",
    "        display(render_tensor([out[\"image\"].clamp(-1, 1), pred_out[\"image\"].clamp(-1, 1)], nrow=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d23e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8c05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerffaceshop",
   "language": "python",
   "name": "nerffaceshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
